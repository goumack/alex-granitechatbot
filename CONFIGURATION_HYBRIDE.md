# Configuration Hybride: Ollama + NVIDIA NIM

## Architecture

ALEX utilise maintenant une **configuration hybride** optimale:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ALEX Application                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  Embeddings â”€â”€â–º Ollama                 â”‚
â”‚  (nomic-embed-text)                    â”‚
â”‚  â””â”€ URL: ollamaaccel.senum.africa     â”‚
â”‚                                         â”‚
â”‚  Chat â”€â”€â–º NVIDIA NIM                   â”‚
â”‚  (meta/llama-3.2-3b-instruct)          â”‚
â”‚  â””â”€ URL: integrate.api.nvidia.com     â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Pourquoi cette configuration?

### Embeddings avec Ollama
âœ… **Nomic-embed-text** est optimisÃ© pour votre cas d'usage
âœ… Infrastructure Ollama dÃ©jÃ  en place et testÃ©e
âœ… Pas de coÃ»t d'API pour les embeddings
âœ… Faible latence avec serveur local

### Chat avec NVIDIA NIM
âœ… **Meta Llama 3.2** offre une meilleure qualitÃ© de rÃ©ponse
âœ… Infrastructure cloud scalable
âœ… Pas besoin de maintenir un serveur Ollama pour le chat
âœ… API REST moderne avec SDK OpenAI

## Configuration (.env)

```bash
# Ollama pour embeddings
OLLAMA_BASE_URL=https://ollamaaccel-chatbotaccel.apps.senum.heritage.africa
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# NVIDIA NIM pour chat
NVIDIA_API_KEY=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
NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1
NVIDIA_CHAT_MODEL=meta/llama-3.2-3b-instruct

# ChromaDB
CHROMA_PERSIST_DIRECTORY=./chroma_db

# Application
APP_TITLE=ALEX - Assistant IA avec RAG
APP_DESCRIPTION=Chatbot intelligent avec recherche dans la base de connaissances
```

## Flux de donnÃ©es

### 1. Indexation de documents
```
Document â”€â”€â–º Chunking â”€â”€â–º Ollama (embeddings) â”€â”€â–º ChromaDB
```

### 2. RequÃªte utilisateur
```
Question â”€â”€â–º Ollama (embedding) â”€â”€â–º ChromaDB (search) â”€â”€â–º Context
                                                            â”‚
                                                            â–¼
User â—„â”€â”€ NVIDIA NIM (gÃ©nÃ©ration) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Context + Question
```

## Code Principal

### GÃ©nÃ©ration d'embeddings (Ollama)
```python
def generate_embeddings(self, text: str) -> List[float]:
    """GÃ©nÃ¨re des embeddings avec Ollama (nomic-embed-text)"""
    payload = {
        "model": self.config.OLLAMA_EMBEDDING_MODEL,
        "prompt": text
    }

    response = self._session.post(
        f"{self.config.OLLAMA_BASE_URL}/api/embeddings",
        json=payload,
        timeout=30
    )

    return response.json()['embedding']
```

### GÃ©nÃ©ration de chat (NVIDIA NIM)
```python
def chat(self, message: str) -> str:
    """GÃ©nÃ¨re une rÃ©ponse avec NVIDIA NIM"""
    # 1. Chercher le contexte avec embeddings Ollama
    context = self.search_context(message, limit=5)

    # 2. GÃ©nÃ©rer la rÃ©ponse avec NVIDIA NIM
    completion = self.nvidia_client.chat.completions.create(
        model=self.config.NVIDIA_CHAT_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
        top_p=0.7,
        max_tokens=1024,
        stream=False
    )

    return completion.choices[0].message.content
```

## Health Check

L'endpoint `/health` vÃ©rifie les deux services:

```json
{
  "nvidia_status": "ğŸŸ¢ ConnectÃ©",
  "ollama_status": "ğŸŸ¢ ConnectÃ©",
  "nvidia_url": "https://integrate.api.nvidia.com/v1",
  "ollama_url": "https://ollamaaccel-chatbotaccel.apps.senum.heritage.africa",
  "timestamp": "2025-11-27 09:30:00"
}
```

## DÃ©marrage

```bash
# VÃ©rifier les dÃ©pendances
pip install openai requests chromadb

# Lancer l'application
python "taipy_version/app_taipy copy 4.py"
```

Sortie attendue:
```
   DÃ©marrage d'ALEX...
==================================================
ğŸ”— Configuration Hybride:
   Chat (NVIDIA NIM): meta/llama-3.2-3b-instruct
   Embeddings (Ollama): nomic-embed-text
   Ollama URL: https://ollamaaccel-chatbotaccel.apps.senum.heritage.africa
   RÃ©pertoire surveillÃ©: ./documents
ğŸŒ DÃ©marrage de l'interface...
```

## Avantages de cette architecture

### Performance
- âš¡ Embeddings rapides avec Ollama local
- âš¡ Chat optimisÃ© avec NVIDIA cloud
- âš¡ Pas de goulot d'Ã©tranglement

### CoÃ»t
- ğŸ’° Embeddings gratuits (Ollama)
- ğŸ’° Chat avec quota NVIDIA
- ğŸ’° RÃ©duction des coÃ»ts vs 100% cloud

### FiabilitÃ©
- ğŸ”’ Embeddings indÃ©pendants de l'API NVIDIA
- ğŸ”’ Fallback possible sur chaque composant
- ğŸ”’ Double redondance

### MaintenabilitÃ©
- ğŸ› ï¸ Chaque composant peut Ãªtre mis Ã  jour indÃ©pendamment
- ğŸ› ï¸ Tests sÃ©parÃ©s pour embeddings et chat
- ğŸ› ï¸ Configuration claire et modulaire

## Migration future

Si besoin de changer un composant:

### Remplacer Ollama par NVIDIA pour embeddings
```python
# Dans .env
NVIDIA_EMBEDDING_MODEL=nvidia/nv-embedqa-e5-v5

# Dans le code
response = self.nvidia_client.embeddings.create(
    input=[text],
    model=self.config.NVIDIA_EMBEDDING_MODEL,
    encoding_format="float",
    extra_body={"input_type": "query", "truncate": "NONE"}
)
```

### Remplacer NVIDIA par Ollama pour chat
```python
# Dans .env
OLLAMA_CHAT_MODEL=llama3

# Dans le code
response = requests.post(
    f"{self.config.OLLAMA_BASE_URL}/api/generate",
    json={"model": self.config.OLLAMA_CHAT_MODEL, "prompt": prompt}
)
```

## Monitoring

VÃ©rifier les logs pour voir quelle API est utilisÃ©e:

```
ğŸ”„ Tentative 1/3 embedding Ollama (timeout: 30s)
âœ… Embedding gÃ©nÃ©rÃ© avec succÃ¨s (tentative 1)
   PROMPT ENVOYÃ‰ Ã€ NVIDIA:
   RÃ‰PONSE NVIDIA: ...
```

## Support

En cas de problÃ¨me:
1. VÃ©rifier `/health` pour le statut des services
2. VÃ©rifier les logs pour identifier quel service est en erreur
3. Tester individuellement Ollama et NVIDIA
